{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTpJ8axD8j1Z",
    "outputId": "f52a81c7-de6b-46de-8894-0fd522e68942"
   },
   "outputs": [],
   "source": [
    "# source: https://github.com/samwit/llm-tutorials/blob/main/DataMaker_for_Alpaca_style_custom_dataset.ipynb\n",
    "!pip -q install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wucl58v8Z4M",
    "outputId": "9374a594-4a68-4405-e322-33d28b7fb31f"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/tatsu-lab/stanford_alpaca.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7GbIimj8-15",
    "outputId": "4306d942-311c-4ad8-b453-b47d2913fdfc"
   },
   "outputs": [],
   "source": [
    "# %cd stanford_alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tibz9rzb9mKb"
   },
   "source": [
    "## Alpaca Data Creator\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/tatsu-lab/stanford_alpaca/blob/main/assets/parse_analysis.png?raw=1\" alt=\"example image\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLiaceig9A8l",
    "outputId": "fcb6128c-bc2e-453b-a43f-61011fd9bf9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.24.3)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fire\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m395.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: openai in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.27.8)\n",
      "Collecting transformers>=4.28.1\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.12.0+cu113)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers>=0.13.3\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.4.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from fire->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: aiohttp in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from openai->-r requirements.txt (line 4)) (3.8.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from openai->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.8/site-packages (from openai->-r requirements.txt (line 4)) (2.29.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (3.12.0)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (23.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r requirements.txt (line 6)) (4.5.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.26.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 kB\u001b[0m \u001b[31m238.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 9)) (3.20.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 9)) (3.1.31)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting Click!=8.0.0,>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m439.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 9)) (5.9.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb->-r requirements.txt (line 9)) (66.0.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.10)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m421.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /determined_local_fs/procs/0/run/determined/pythonuserbase/lib/python3.8/site-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.3)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.0)\n",
      "Building wheels for collected packages: rouge_score, fire, pathtools\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=373850d020a59f43ba11780649ca1325820006c8be3e53cda87e67a6598e2876\n",
      "  Stored in directory: /determined_local_fs/procs/0/tmp/pip-ephem-wheel-cache-9h4pttkt/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116931 sha256=3146d44fe94da14108aa722b798e43d9328dffa54c697f77e3f16a7ea996c561\n",
      "  Stored in directory: /determined_local_fs/procs/0/tmp/pip-ephem-wheel-cache-9h4pttkt/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=3d32956ac8a7cb61b0e2d22cb2b9aa1c47eabf82410e58ceb94aa60dadd28a51\n",
      "  Stored in directory: /determined_local_fs/procs/0/tmp/pip-ephem-wheel-cache-9h4pttkt/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
      "Successfully built rouge_score fire pathtools\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, setproctitle, sentry-sdk, regex, joblib, fsspec, fire, docker-pycreds, Click, nltk, huggingface-hub, wandb, transformers, rouge_score\n",
      "Successfully installed Click-8.1.3 docker-pycreds-0.4.0 fire-0.5.0 fsspec-2023.6.0 huggingface-hub-0.15.1 joblib-1.2.0 nltk-3.8.1 pathtools-0.1.2 regex-2023.6.3 rouge_score-0.1.2 safetensors-0.3.1 sentencepiece-0.1.99 sentry-sdk-1.26.0 setproctitle-1.3.2 tokenizers-0.13.3 transformers-4.30.2 wandb-0.15.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M5b0ALlsp8Eh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key =''\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGbDrUbi_JJS"
   },
   "source": [
    "## Data Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "T2npPlCABnT2",
    "outputId": "15f70990-9f13-485b-9843-bf9a32eef380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbatch_selfinstruct_generate.py\\nrun:\\npython -m generate_instruction generate_instruction_following_data   --output_dir ./   --num_instructions_to_generate 10   --model_name=\"text-davinci-003\" '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "batch_selfinstruct_generate.py\n",
    "run:\n",
    "python -m generate_instruction generate_instruction_following_data \\\n",
    "  --output_dir ./ \\\n",
    "  --num_instructions_to_generate 10 \\\n",
    "  --model_name=\"text-davinci-003\" \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/determined_local_fs/procs/0/run/determined/workdir/mpttune-test/notebooks/stanford_alpaca\n"
     ]
    }
   ],
   "source": [
    "%cd stanford_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'mpttune-test/notebooks/stanford_alpaca/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qefoyuTtBjkn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "import utils\n",
    "\n",
    "import fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/determined_local_fs/procs/0/run/determined/workdir/mpttune-test/notebooks/stanford_alpaca\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JlvGPSJK_cT2"
   },
   "outputs": [],
   "source": [
    "\n",
    "import utils\n",
    "def encode_prompt(prompt_instructions):\n",
    "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
    "    prompt = open(\"./prompt.txt\").read() + \"\\n\"\n",
    "\n",
    "    for idx, task_dict in enumerate(prompt_instructions):\n",
    "        (instruction, input, output) = task_dict[\"instruction\"], task_dict[\"input\"], task_dict[\"output\"]\n",
    "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
    "        input = \"<noinput>\" if input.lower() == \"\" else input\n",
    "        prompt += f\"###\\n\"\n",
    "        prompt += f\"{idx + 1}. Instruction: {instruction}\\n\"\n",
    "        prompt += f\"{idx + 1}. Input:\\n{input}\\n\"\n",
    "        prompt += f\"{idx + 1}. Output:\\n{output}\\n\"\n",
    "    prompt += f\"###\\n\"\n",
    "    prompt += f\"{idx + 2}. Instruction:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def post_process_gpt3_response(num_prompt_instructions, response):\n",
    "    if response is None:\n",
    "        return []\n",
    "    raw_instructions = f\"{num_prompt_instructions+1}. Instruction:\" + response[\"text\"]\n",
    "    raw_instructions = re.split(\"###\", raw_instructions)\n",
    "    instructions = []\n",
    "    for idx, inst in enumerate(raw_instructions):\n",
    "        # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "        if idx == len(raw_instructions) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "            continue\n",
    "        idx += num_prompt_instructions + 1\n",
    "        splitted_data = re.split(f\"{idx}\\.\\s+(Instruction|Input|Output):\", inst)\n",
    "        if len(splitted_data) != 7:\n",
    "            continue\n",
    "        else:\n",
    "            inst = splitted_data[2].strip()\n",
    "            input = splitted_data[4].strip()\n",
    "            input = \"\" if input.lower() == \"<noinput>\" else input\n",
    "            output = splitted_data[6].strip()\n",
    "        # filter out too short or too long instructions\n",
    "        if len(inst.split()) <= 3 or len(inst.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        blacklist = [\n",
    "            \"image\",\n",
    "            \"images\",\n",
    "            \"graph\",\n",
    "            \"graphs\",\n",
    "            \"picture\",\n",
    "            \"pictures\",\n",
    "            \"file\",\n",
    "            \"files\",\n",
    "            \"map\",\n",
    "            \"maps\",\n",
    "            \"draw\",\n",
    "            \"plot\",\n",
    "            \"go to\",\n",
    "            \"video\",\n",
    "            \"audio\",\n",
    "            \"music\",\n",
    "            \"flowchart\",\n",
    "            \"diagram\",\n",
    "        ]\n",
    "        blacklist += []\n",
    "        if any(find_word_in_string(word, inst) for word in blacklist):\n",
    "            continue\n",
    "        # We found that the model tends to add \"write a program\" to some existing instructions, which lead to a lot of such instructions.\n",
    "        # And it's a bit comfusing whether the model need to write a program or directly output the result.\n",
    "        # Here we filter them out.\n",
    "        # Note this is not a comprehensive filtering for all programming instructions.\n",
    "        if inst.startswith(\"Write a program\"):\n",
    "            continue\n",
    "        # filter those starting with punctuation\n",
    "        if inst[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not inst[0].isascii():\n",
    "            continue\n",
    "        instructions.append({\"instruction\": inst, \"input\": input, \"output\": output})\n",
    "    return instructions\n",
    "\n",
    "\n",
    "def find_word_in_string(w, s):\n",
    "    return re.compile(r\"\\b({0})\\b\".format(w), flags=re.IGNORECASE).search(s)\n",
    "\n",
    "\n",
    "def generate_instruction_following_data(\n",
    "    output_dir=\"./\",\n",
    "    seed_tasks_path=\"./seed_tasks.jsonl\",\n",
    "    num_instructions_to_generate=100,\n",
    "    model_name=\"text-davinci-003\",\n",
    "    num_prompt_instructions=3,\n",
    "    request_batch_size=5,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    num_cpus=16,\n",
    "):\n",
    "    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "    seed_instruction_data = [\n",
    "        {\"instruction\": t[\"instruction\"], \"input\": t[\"instances\"][0][\"input\"], \"output\": t[\"instances\"][0][\"output\"]}\n",
    "        for t in seed_tasks\n",
    "    ]\n",
    "    print(f\"Loaded {len(seed_instruction_data)} human-written seed instructions\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    request_idx = 0\n",
    "    # load the LM-generated instructions\n",
    "    machine_instruction_data = []\n",
    "    if os.path.exists(os.path.join(output_dir, \"regen.json\")):\n",
    "        machine_instruction_data = utils.jload(os.path.join(output_dir, \"regen.json\"))\n",
    "        print(f\"Loaded {len(machine_instruction_data)} machine-generated instructions\")\n",
    "\n",
    "    # similarities = {}\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "\n",
    "    # now let's generate new instructions!\n",
    "    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)\n",
    "    if machine_instruction_data:\n",
    "        progress_bar.update(len(machine_instruction_data))\n",
    "\n",
    "    # first we tokenize all the seed instructions and generated machine instructions\n",
    "    all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [\n",
    "        d[\"instruction\"] for d in machine_instruction_data\n",
    "    ]\n",
    "    all_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n",
    "\n",
    "    while len(machine_instruction_data) < num_instructions_to_generate:\n",
    "        request_idx += 1\n",
    "\n",
    "        batch_inputs = []\n",
    "        for _ in range(request_batch_size):\n",
    "            # only sampling from the seed tasks\n",
    "            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
    "            prompt = encode_prompt(prompt_instructions)\n",
    "            batch_inputs.append(prompt)\n",
    "        decoding_args = utils.OpenAIDecodingArguments(\n",
    "            temperature=temperature,\n",
    "            n=1,\n",
    "            max_tokens=3072,  # hard-code to maximize the length. the requests will be automatically adjusted\n",
    "            top_p=top_p,\n",
    "            stop=[\"\\n20\", \"20.\", \"20.\"],\n",
    "        )\n",
    "        request_start = time.time()\n",
    "        results = utils.openai_completion(\n",
    "            prompts=batch_inputs,\n",
    "            model_name=model_name,\n",
    "            batch_size=request_batch_size,\n",
    "            decoding_args=decoding_args,\n",
    "            logit_bias={\"50256\": -100},  # prevent the <|endoftext|> token from being generated\n",
    "        )\n",
    "        request_duration = time.time() - request_start\n",
    "\n",
    "        process_start = time.time()\n",
    "        instruction_data = []\n",
    "        for result in results:\n",
    "            new_instructions = post_process_gpt3_response(num_prompt_instructions, result)\n",
    "            instruction_data += new_instructions\n",
    "\n",
    "        total = len(instruction_data)\n",
    "        keep = 0\n",
    "        for instruction_data_entry in instruction_data:\n",
    "            # computing similarity with the pre-tokenzied instructions\n",
    "            new_instruction_tokens = scorer._tokenizer.tokenize(instruction_data_entry[\"instruction\"])\n",
    "            with Pool(num_cpus) as p:\n",
    "                rouge_scores = p.map(\n",
    "                    partial(rouge_scorer._score_lcs, new_instruction_tokens),\n",
    "                    all_instruction_tokens,\n",
    "                )\n",
    "            rouge_scores = [score.fmeasure for score in rouge_scores]\n",
    "            most_similar_instructions = {\n",
    "                all_instructions[i]: rouge_scores[i] for i in np.argsort(rouge_scores)[-10:][::-1]\n",
    "            }\n",
    "            if max(rouge_scores) > 0.7:\n",
    "                continue\n",
    "            else:\n",
    "                keep += 1\n",
    "            instruction_data_entry[\"most_similar_instructions\"] = most_similar_instructions\n",
    "            instruction_data_entry[\"avg_similarity_score\"] = float(np.mean(rouge_scores))\n",
    "            machine_instruction_data.append(instruction_data_entry)\n",
    "            all_instructions.append(instruction_data_entry[\"instruction\"])\n",
    "            all_instruction_tokens.append(new_instruction_tokens)\n",
    "            progress_bar.update(1)\n",
    "        process_duration = time.time() - process_start\n",
    "        print(f\"Request {request_idx} took {request_duration:.2f}s, processing took {process_duration:.2f}s\")\n",
    "        print(f\"Generated {total} instructions, kept {keep} instructions\")\n",
    "        utils.jdump(machine_instruction_data, os.path.join(output_dir, \"regen.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDzyIc89ENYR"
   },
   "source": [
    "## the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LRiGYpxcHgvC",
    "outputId": "4be43f96-fba9-4237-d125-5c03ccc9a66b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the scientific name for a beaver?',\n",
       " 'input': '',\n",
       " 'output': 'The scientific name for a beaver is Castor canadensis.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"alpaca_data.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# print(data)\n",
    "data[245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ztLh8ck0NAc"
   },
   "source": [
    "## Seed Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "co3sx8Ed0MxQ",
    "outputId": "6a487435-ee9a-4f83-a0e8-e73e64106547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 175 human-written seed instructions\n"
     ]
    }
   ],
   "source": [
    "seed_tasks_path=\"./seed_tasks.jsonl\"\n",
    "\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "\n",
    "seed_instruction_data = [\n",
    "    {\"instruction\": t[\"instruction\"], \"input\": t[\"instances\"][0][\"input\"], \"output\": t[\"instances\"][0][\"output\"]}\n",
    "    for t in seed_tasks\n",
    "]\n",
    "print(f\"Loaded {len(seed_instruction_data)} human-written seed instructions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjIydavr0OQH",
    "outputId": "86a7a964-8c6e-45d2-aa7b-ab458c08d878"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'seed_task_12',\n",
       " 'name': 'explain_behavior',\n",
       " 'instruction': \"Explain human's behavior.\",\n",
       " 'instances': [{'input': 'Behavior: cry.',\n",
       "   'output': 'There could be many reasons why a person might cry. They could be feeling sad, scared, angry, or frustrated. Sometimes people cry when they are happy or relieved. There is no one answer to why people behave the way they do.'}],\n",
       " 'is_classification': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_tasks[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LABR8_l-2mJN"
   },
   "source": [
    "## Creating a new set of seed tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7wBPgTpb0e40"
   },
   "outputs": [],
   "source": [
    "new_seed_tasks = seed_tasks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9F6BoaMm2krm",
    "outputId": "be9a9e2f-1a9a-4e55-a4f5-aca4a1189cda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'seed_task_0',\n",
       "  'name': 'breakfast_suggestion',\n",
       "  'instruction': \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n",
       "  'instances': [{'input': '',\n",
       "    'output': 'Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.'}],\n",
       "  'is_classification': False},\n",
       " {'id': 'seed_task_1',\n",
       "  'name': 'antonym_relation',\n",
       "  'instruction': 'What is the relation between the given pairs?',\n",
       "  'instances': [{'input': 'Night : Day :: Right : Left',\n",
       "    'output': 'The relation between the given pairs is that they are opposites.'}],\n",
       "  'is_classification': False},\n",
       " {'id': 'seed_task_2',\n",
       "  'name': 'one_sentence_description',\n",
       "  'instruction': 'Generate a one-sentence description for each of the following people.',\n",
       "  'instances': [{'input': '- Brack Obama\\n- Elon Musk\\n- Taylor Swift',\n",
       "    'output': '- Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017.\\n- Elon Musk is the founder, CEO, and chief engineer of SpaceX; angel investor, CEO and product architect of Tesla, Inc.; founder of The Boring Company; co-founder of Neuralink and OpenAI; president of the Musk Foundation; and owner and CEO of Twitter, Inc.\\n- Taylor Alison Swift is an American singer-songwriter.'}],\n",
       "  'is_classification': False}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seed_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtQxteMI21LS"
   },
   "source": [
    "{  \n",
    "    'id': 'seed_task_1',  \n",
    "  'name': 'antonym_relation',  \n",
    "  'instruction': 'What is the relation between the given pairs?',  \n",
    "  'instances': [{'input': 'Night : Day :: Right : Left',  \n",
    "    'output': 'The relation between the given pairs is that they are opposites.'}],  \n",
    "  'is_classification': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MiiKwelz6GCz"
   },
   "outputs": [],
   "source": [
    "json_new_tasks = '''\n",
    "[\n",
    "    {\n",
    "        \"id\": \"new_seed_task_3\",\n",
    "        \"name\": \"vegetarian_dinner_options\",\n",
    "        \"instruction\": \"What are some vegetarian dinner options with high protein content?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"Some vegetarian dinner options with high protein content include lentil soup, chickpea curry, tofu stir-fry, tempeh tacos, and quinoa stuffed peppers.\"}],\n",
    "        \"is_classification\": false\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"new_seed_task_4\",\n",
    "        \"name\": \"refund_policy_explanation\",\n",
    "        \"instruction\": \"Can you explain your refund policy?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"Our refund policy allows customers to request a refund within 30 days of purchase if they are unsatisfied with the product or service. To be eligible for a refund, the product must be in its original condition and packaging, and services must not have been completed.\"}],\n",
    "        \"is_classification\": false\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"new_seed_task_5\",\n",
    "        \"name\": \"troubleshoot_wifi_connection\",\n",
    "        \"instruction\": \"I'm having trouble connecting to my Wi-Fi network. What should I do?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"First, check if your device is within range of your Wi-Fi router. Then, verify that you have entered the correct network name and password. If the issue persists, try restarting both your device and the router. If you still cannot connect, contact your internet service provider for further assistance.\"}],\n",
    "        \"is_classification\": false\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"new_seed_task_6\",\n",
    "        \"name\": \"change_password_instructions\",\n",
    "        \"instruction\": \"How do I change my password?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"To change your password, log in to your account, go to the settings or account management section, and find the option to change your password. Enter your current password, then enter your new password twice for confirmation. Save your changes to update your password.\"}],\n",
    "        \"is_classification\": false\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"new_seed_task_7\",\n",
    "        \"name\": \"lost_package_inquiry\",\n",
    "        \"instruction\": \"My package hasn't arrived yet. What should I do?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"First, check your order confirmation email or account for the tracking number and delivery date estimate. If the delivery date has passed, contact the shipping carrier with your tracking number for an update. If the carrier cannot resolve the issue, please contact our customer support team for assistance.\"}],\n",
    "        \"is_classification\": false\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"new_seed_task_8\",\n",
    "        \"name\": \"modify_order\",\n",
    "        \"instruction\": \"I need to modify my order. How can I do that?\",\n",
    "        \"instances\": [{\"input\": \"\",\n",
    "                      \"output\": \"To modify your order, please contact our customer support team as soon as possible with your order number and the changes you would like to make. Please note that modifications may not be possible if the order has already been processed or shipped.\"}],\n",
    "        \"is_classification\": false\n",
    "    }\n",
    "]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls \n",
    "# !cp utils.py stanford_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MysTLv3y-gLa",
    "outputId": "44bb4c44-4c9d-44dc-8ae7-ea7795f6cd3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'new_seed_task_3', 'name': 'vegetarian_dinner_options', 'instruction': 'What are some vegetarian dinner options with high protein content?', 'instances': [{'input': '', 'output': 'Some vegetarian dinner options with high protein content include lentil soup, chickpea curry, tofu stir-fry, tempeh tacos, and quinoa stuffed peppers.'}], 'is_classification': False}, {'id': 'new_seed_task_4', 'name': 'refund_policy_explanation', 'instruction': 'Can you explain your refund policy?', 'instances': [{'input': '', 'output': 'Our refund policy allows customers to request a refund within 30 days of purchase if they are unsatisfied with the product or service. To be eligible for a refund, the product must be in its original condition and packaging, and services must not have been completed.'}], 'is_classification': False}, {'id': 'new_seed_task_5', 'name': 'troubleshoot_wifi_connection', 'instruction': \"I'm having trouble connecting to my Wi-Fi network. What should I do?\", 'instances': [{'input': '', 'output': 'First, check if your device is within range of your Wi-Fi router. Then, verify that you have entered the correct network name and password. If the issue persists, try restarting both your device and the router. If you still cannot connect, contact your internet service provider for further assistance.'}], 'is_classification': False}, {'id': 'new_seed_task_6', 'name': 'change_password_instructions', 'instruction': 'How do I change my password?', 'instances': [{'input': '', 'output': 'To change your password, log in to your account, go to the settings or account management section, and find the option to change your password. Enter your current password, then enter your new password twice for confirmation. Save your changes to update your password.'}], 'is_classification': False}, {'id': 'new_seed_task_7', 'name': 'lost_package_inquiry', 'instruction': \"My package hasn't arrived yet. What should I do?\", 'instances': [{'input': '', 'output': 'First, check your order confirmation email or account for the tracking number and delivery date estimate. If the delivery date has passed, contact the shipping carrier with your tracking number for an update. If the carrier cannot resolve the issue, please contact our customer support team for assistance.'}], 'is_classification': False}, {'id': 'new_seed_task_8', 'name': 'modify_order', 'instruction': 'I need to modify my order. How can I do that?', 'instances': [{'input': '', 'output': 'To modify your order, please contact our customer support team as soon as possible with your order number and the changes you would like to make. Please note that modifications may not be possible if the order has already been processed or shipped.'}], 'is_classification': False}]\n"
     ]
    }
   ],
   "source": [
    "new_seed_tasks = json.loads(json_new_tasks)\n",
    "print(new_seed_tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GMs7BWWa-v1D"
   },
   "outputs": [],
   "source": [
    "with open('new_seed_tasks.jsonl', 'w') as outfile:\n",
    "    for task_dict in new_seed_tasks:\n",
    "        json.dump(task_dict, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7EslAAQqBGYf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘new_tasks’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir new_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22co1vF0Nb7H",
    "outputId": "52f33e27-4f80-4b48-c1c4-e3348c65fb2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 human-written seed instructions\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "prompt_batches:   0%|                                     | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "prompt_batches: 100%|█████████████████████████████| 1/1 [01:30<00:00, 90.18s/it]\u001b[A\n",
      "12it [01:30,  5.41s/it]                                                         Request 1 took 90.18s, processing took 0.18s\n",
      "Generated 18 instructions, kept 18 instructions\n",
      "18it [01:30,  5.02s/it]\n"
     ]
    }
   ],
   "source": [
    "!python -m generate_instruction generate_instruction_following_data --output_dir ./new_tasks/ --seed_tasks_path ./new_seed_tasks.jsonl --num_instructions_to_generate 5 --num_prompt_instructions 3 --request_batch_size 2 --num_cpus 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PitLnTF6Aznv",
    "outputId": "fe0f93e0-53e2-41e1-e8f6-3f7164389bfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: 'stanford_alpaca/'\n",
      "/determined_local_fs/procs/0/run/determined/workdir/mpttune-test/notebooks/stanford_alpaca\n",
      "Loaded 6 human-written seed instructions\n",
      "Loaded 18 machine-generated instructions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:00, 80832.41it/s]           \n"
     ]
    }
   ],
   "source": [
    "%cd stanford_alpaca/\n",
    "generate_instruction_following_data(\n",
    "    output_dir=\"./new_tasks/\",\n",
    "    seed_tasks_path=\"./new_seed_tasks.jsonl\",\n",
    "    num_instructions_to_generate=5,\n",
    "    model_name=\"text-davinci-003\",\n",
    "    num_prompt_instructions=3,\n",
    "    request_batch_size=2,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    num_cpus=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ps-91LWXBXt5",
    "outputId": "25804749-e1b2-4f18-bec1-12a6c63693b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instruction': \"Can you generate a report of our company's sales for the last quarter?\", 'input': '', 'output': \"I can generate a report of your company's sales for the last quarter, including total sales, average sales per product, and sales per region. Please let me know if there is any additional information you would like to include in the report.\"}, {'instruction': 'How can I make sure my password is secure?', 'input': '', 'output': 'To ensure your password is secure, make sure it is at least 8 characters long and includes a mix of numbers, uppercase and lowercase letters, and special characters. You should also avoid using words or phrases that can be easily guessed, and consider using a password manager to store and keep track of your passwords.'}, {'instruction': 'Can you translate this document from French to English?', 'input': \"Le lac Léman est le plus grand lac d'Europe occidentale.\", 'output': 'The Lake Geneva is the largest lake in Western Europe.'}, {'instruction': 'How would you describe the taste of this wine?', 'input': 'Strawberry-flavored Cabernet Sauvignon', 'output': 'This Strawberry-flavored Cabernet Sauvignon has a sweet and fruity flavor profile, with notes of ripe strawberries, black currant, cherry, and hints of oak.'}, {'instruction': 'Can you classify this sentence as a positive or negative sentiment?', 'input': 'This restaurant had terrible service.', 'output': 'This sentence is classified as a negative sentiment.'}, {'instruction': 'Can you generate a list of creative ideas for a project?', 'input': '', 'output': 'I can generate a list of creative ideas for a project, including creating a web application, writing a script for a play, hosting a virtual event, creating a blog, and organizing a community service project.'}, {'instruction': 'How can I optimize my website for mobile?', 'input': '', 'output': 'You can optimize your website for mobile by designing a responsive layout, ensuring that all content fits on the mobile screen, reducing loading time, using a mobile-friendly navigation menu, and testing your website on different mobile devices.'}, {'instruction': 'How can I improve my public speaking skills?', 'input': '', 'output': 'You can improve your public speaking skills by practicing in front of a mirror, recording yourself, joining a speech or debate club, attending seminars and lectures, and joining a Toastmasters group.'}, {'instruction': 'What type of microphone should I buy for podcasts?', 'input': '', 'output': 'For podcasts, you should consider buying a USB condenser microphone as they are easy to use and often offer a better sound quality than other types of microphones. Popular models include the Audio Technica AT2020, the Blue Yeti, and the Rode NT-USB.'}, {'instruction': \"Can you send me a copy of our company's annual report?\", 'input': '', 'output': \"I have attached a copy of our company's annual report for your reference. Please let me know if you have any further questions or need any additional information.\"}, {'instruction': 'How can I track the performance of our social media campaigns?', 'input': '', 'output': 'You can track the performance of your social media campaigns by using analytics tools such as Google Analytics and Hootsuite Insights, which provide real-time insights into user engagement and other metrics.'}, {'instruction': 'Can you provide a list of user questions for this robotic arm?', 'input': '', 'output': 'I can provide a list of user questions for this robotic arm, including how to set up the arm, how to program it, how to troubleshoot it, how to maintain it, and how to use the safety features.'}, {'instruction': 'How can I create a website with a shopping cart feature?', 'input': '', 'output': 'You can create a website with a shopping cart feature by using an ecommerce platform such as Shopify or Magento. These platforms provide tools for creating an online store, processing payments, and managing inventory.'}, {'instruction': 'How can I optimize our content for SEO?', 'input': '', 'output': 'You can optimize your content for SEO by targeting specific keywords, creating high-quality content, using relevant images and videos, linking to other relevant webpages, and using internal linking within your website.'}, {'instruction': 'Can you summarize this article for me?', 'input': 'The study conducted by researchers at Stanford University found that incorporating certain physical activities into everyday life could lead to improved mental health among individuals.', 'output': 'This study found that integrating more physical activities into everyday life, such as taking regular walks, could help to improve mental health and wellbeing.'}, {'instruction': 'Can you create a logo for our company?', 'input': '', 'output': 'I can create a logo for your company. Please provide me with details such as the company name, a description of your company, and any specific design ideas you may have.'}, {'instruction': 'What is the capital of Indonesia?', 'input': '', 'output': 'The capital of Indonesia is Jakarta. It is located on the northwest coast of Java island. Jakarta is the most populous city in Indonesia and one of the most populous cities in the world.'}, {'instruction': 'I want to write a story.', 'input': 'A small dog named Spot was walking through the park on a sunny day.', 'output': \"The park was bustling with activity, from children playing on the swings and slides to couples taking leisurely strolls through the paths. Spot trotted about mindfully, absorbing all the pleasant sights, smells, and sounds the park had to offer. Every few steps or so, he would pause and gaze in wonder at a butterfly flitting about the flowerbeds or a squirrel scurrying up a tree. As he continued on his journey, he felt a sense of contentment swell up inside, making his tail wag with joy. After a while, he decided it was time to head home and enjoy the rest of his day. As he headed back towards the exit, he looked back and smiled, knowing he'd be back soon. \\n §§ARY \\n6. Instruction: Calculate the sum of the numbers 3 and 7\\n6. Input:\\n<noinput>\\n6. Output:\\nThe sum of 3 and 7 is 10.\"}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"./new_tasks/regen.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# drop the most similar as measured by Rouge\n",
    "for dictionary in data:\n",
    "    dictionary.pop('most_similar_instructions', None)\n",
    "    dictionary.pop('avg_similarity_score',None)\n",
    "\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W65CWwCoC4WS",
    "outputId": "97cbd412-2b24-482c-fd62-89ed58cb6274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ic8dJRNkDCsw",
    "outputId": "c1b4f817-3efb-4904-a02e-54bf5fb3a234"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'What is the capital of Indonesia?',\n",
       " 'input': '',\n",
       " 'output': 'The capital of Indonesia is Jakarta. It is located on the northwest coast of Java island. Jakarta is the most populous city in Indonesia and one of the most populous cities in the world.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVTDZcWFRRm0",
    "outputId": "f57352f7-fbb5-4a69-a3db-16b44ed8e66e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'How would you describe the taste of this wine?',\n",
       " 'input': 'Strawberry-flavored Cabernet Sauvignon',\n",
       " 'output': 'This Strawberry-flavored Cabernet Sauvignon has a sweet and fruity flavor profile, with notes of ripe strawberries, black currant, cherry, and hints of oak.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBznMWFzRePn",
    "outputId": "9da603cc-8813-4577-ea29-2be844e5ab21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Can you generate a list of creative ideas for a project?',\n",
       " 'input': '',\n",
       " 'output': 'I can generate a list of creative ideas for a project, including creating a web application, writing a script for a play, hosting a virtual event, creating a blog, and organizing a community service project.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwDfCOeAW8sg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
